{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sberbank Russian Housing Market - Kaggle\n",
    "### 25 June, 2017\n",
    "# Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use price/sqm for full_sq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Variables specific for competition\n",
    "\n",
    "DATE = '170626'                                      # included in the name of submit files and other output files\n",
    "\n",
    "ID = 'id'                                            \n",
    "TARGET = 'price_doc'                                 \n",
    "LOGTARGET = 'log_price_doc'                         \n",
    "DIRECTORY = 'D:/J/Kaggle/6_Sberbank_HousingPrices_2017/'     \n",
    "\n",
    "# The final training, validation and test set are saved into these files:\n",
    "TRAIN_PREPROCESSED = DIRECTORY + 'data/train_preprocessed.csv'    \n",
    "VALID_PREPROCESSED = DIRECTORY + 'data/valid_preprocessed.csv'    \n",
    "LOCALTEST_PREPROCESSED = DIRECTORY + 'data/localtest_preprocessed.csv'    \n",
    "\n",
    "# The predictions will be made for this dataset:\n",
    "TEST_PREPROCESSED = DIRECTORY + 'data/test_preprocessed.csv'   \n",
    "\n",
    "# The file for the submission:\n",
    "# in the notebook cells the file name will be\n",
    "# SUBMISSION_FILE + 'validscore_num.csv'\n",
    "# where 'validscore' and 'num' refer to score on validation set and prediction respectively\n",
    "SUBMISSION_FILE = DIRECTORY + 'submissions/submit_lgbpy_' + DATE   \n",
    "\n",
    "# First part of file name to save predictions on valid and local test sets (similar to the submission):\n",
    "VALIDPRED_FILE = DIRECTORY + 'predictions/valid_lgbpy_' + DATE     \n",
    "LOCALTESTPRED_FILE = DIRECTORY + 'predictions/localtest_lgbpy_' + DATE\n",
    "\n",
    "# File for recording validation scores:\n",
    "SCORES_FILE = DIRECTORY + 'records/validscores_lgbpy_' + DATE + '.txt' \n",
    "RECORD_SUBMISSION = 'submit_lgbpy_' + DATE     # to record the name of submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TARGET_PER_FULLSQ = 'price_per_fullsq'\n",
    "LOG_TARGET_PER_FULLSQ = 'log_of_price_per_fullsq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "np.random.seed(int(DATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsle(logtarget, logpred):\n",
    "    # logtarget = np.log(target + 1)\n",
    "    # logpred is the prediction of logtarget\n",
    "    return np.sqrt(np.sum(np.square(logpred - logtarget)) / logtarget.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cafe_count_3000_price_1500',\n",
       " 'cafe_count_2000',\n",
       " 'full_sq',\n",
       " 'cafe_count_5000',\n",
       " 'cafe_count_3000',\n",
       " 'sport_count_2000']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_importance = pd.read_csv(DIRECTORY + 'data/important_features_fullsq.csv', header = None)\n",
    "features_importance = list(features_importance.loc[:, 0])\n",
    "features_restr = features_importance[:6]\n",
    "features_restr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: X_train: (21201, 1110), X_valid: (4710, 1110), X_localtest: (4531, 1110), X_test: (7662, 1110)\n",
      "Predicting validation and local test file...\n",
      "<class 'numpy.ndarray'>\n",
      "(4710,)\n",
      "RMSLE on validation set: 0.430532614533\n",
      "Recording scores...\n",
      "Features importance...\n",
      "                                  feature      gain  split\n",
      "0              cafe_count_3000_price_1500  8.288014     74\n",
      "1                         cafe_count_2000  6.370490    115\n",
      "2                                 full_sq  5.943501   1348\n",
      "4                         cafe_count_3000  2.918350     73\n",
      "3                         cafe_count_5000  1.832805    114\n",
      "10                apartment_name_yr_month  1.573582    318\n",
      "5                        sport_count_2000  1.515725     98\n",
      "8               ratio_nonlifesq_to_lifesq  1.473284    446\n",
      "7   raion_build_count_with_builddate_info  1.411538     90\n",
      "9              cafe_count_5000_price_high  1.181969     57\n",
      "6                          apartment_name  1.180948    354\n",
      "14                                  floor  1.127190    573\n",
      "13                           preschool_km  1.091316    271\n",
      "12                         metro_min_avto  1.058609    233\n",
      "11                             nonlife_sq  1.011765    444\n",
      "16             cafe_count_5000_price_2500  0.958276     98\n",
      "17                        kindergarten_km  0.850431    361\n",
      "18                             build_year  0.801026    309\n",
      "24                                life_sq  0.777236    311\n",
      "21                          green_zone_km  0.751655    304\n",
      "20                            railroad_km  0.744388    333\n",
      "19                                  state  0.735304    232\n",
      "22                ratio_kitchsq_to_lifesq  0.671729    162\n",
      "23            public_transport_station_km  0.651573    364\n",
      "36                          workplaces_km  0.649126    316\n",
      "Training model on alltrain dataset and creating predictions to submit...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_PREPROCESSED)\n",
    "valid = pd.read_csv(VALID_PREPROCESSED)\n",
    "localtest = pd.read_csv(LOCALTEST_PREPROCESSED)\n",
    "test = pd.read_csv(TEST_PREPROCESSED)\n",
    "\n",
    "# impute missing full_sq with either life_sq or the median in the neighborhood\n",
    "test.loc[464, 'full_sq'] = 37.8\n",
    "test.loc[5383, 'full_sq'] = 42.07\n",
    "test.loc[6350, 'full_sq'] = test[test['apartment_name'] == test.loc[6350, 'apartment_name']]['full_sq'].median()\n",
    "\n",
    "train[TARGET_PER_FULLSQ] = train[TARGET] / train['full_sq']\n",
    "train[LOG_TARGET_PER_FULLSQ] = np.log(train[TARGET_PER_FULLSQ] + 1)\n",
    "valid[TARGET_PER_FULLSQ] = valid[TARGET] / valid['full_sq']\n",
    "valid[LOG_TARGET_PER_FULLSQ] = np.log(valid[TARGET_PER_FULLSQ] + 1)\n",
    "localtest[TARGET_PER_FULLSQ] = localtest[TARGET] / localtest['full_sq']\n",
    "localtest[LOG_TARGET_PER_FULLSQ] = np.log(localtest[TARGET_PER_FULLSQ] + 1)\n",
    "\n",
    "# drop rows with nan in column LOG_TARGET_PER_LIFESQ\n",
    "train.dropna(subset=[LOG_TARGET_PER_FULLSQ], inplace = True)\n",
    "valid.dropna(subset=[LOG_TARGET_PER_FULLSQ], inplace = True)\n",
    "localtest.dropna(subset=[LOG_TARGET_PER_FULLSQ], inplace = True)\n",
    "\n",
    "train_y = train[LOG_TARGET_PER_FULLSQ]\n",
    "valid_id = valid[ID].copy()\n",
    "valid_y = valid[LOG_TARGET_PER_FULLSQ].copy()\n",
    "localtest_id = localtest[ID]\n",
    "localtest_y = localtest[LOG_TARGET_PER_FULLSQ]\n",
    "alltrain_y = pd.concat([train_y, valid_y, localtest_y], ignore_index = True)\n",
    "test_id = test[ID]\n",
    "\n",
    "train.drop([ID, TARGET, LOGTARGET, TARGET_PER_FULLSQ, LOG_TARGET_PER_FULLSQ, 'timestamp'], axis = 1, inplace = True)\n",
    "valid.drop([ID, TARGET, LOGTARGET, TARGET_PER_FULLSQ, LOG_TARGET_PER_FULLSQ, 'timestamp'], axis = 1, inplace = True)\n",
    "localtest.drop([ID, TARGET, LOGTARGET, TARGET_PER_FULLSQ, LOG_TARGET_PER_FULLSQ, 'timestamp'], axis = 1, inplace = True)\n",
    "test.drop([ID, 'timestamp'], axis = 1, inplace = True)\n",
    "print(\"Data: X_train: {}, X_valid: {}, X_localtest: {}, X_test: {}\".format(train.shape, valid.shape,\n",
    "                                                                           localtest.shape, test.shape))\n",
    "\n",
    "pos1 = train.shape[0]\n",
    "pos2 = train.shape[0] + valid.shape[0]\n",
    "pos3 = train.shape[0] + valid.shape[0] + localtest.shape[0]\n",
    "\n",
    "data = pd.concat([train, valid, localtest, test], ignore_index = True)\n",
    "del train, valid, localtest, test\n",
    "\n",
    "data_num = data.select_dtypes(exclude = ['object'])\n",
    "data_obj = data.select_dtypes(include = ['object']).copy()\n",
    "for c in data_obj:\n",
    "    data_obj[c] = pd.factorize(data_obj[c])[0]\n",
    "\n",
    "data = pd.concat([data_num, data_obj], axis = 1)\n",
    "\n",
    "train = data[ : pos1]\n",
    "valid = data[pos1 : pos2]\n",
    "localtest = data[pos2 : pos3]\n",
    "test = data[pos3 : ]\n",
    "\n",
    "features_restr = features_importance[:300]\n",
    "train_restr = train[features_restr]\n",
    "valid_restr = valid[features_restr]\n",
    "localtest_restr = localtest[features_restr]\n",
    "test_restr = test[features_restr]\n",
    "\n",
    "del train, valid, localtest, test\n",
    "\n",
    "train_lgb = lgb.Dataset(train_restr, train_y)\n",
    "\n",
    "NUM = 170901\n",
    "ROUNDS = 1500\n",
    "params = {\n",
    "\t'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.01,\n",
    "    'verbose': 1,\n",
    "    'num_leaves': 2 ** 5,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_seed': DATE,\n",
    "    'feature_fraction': 0.7,\n",
    "    'feature_fraction_seed': DATE,\n",
    "    'max_bin': 100,\n",
    "    'max_depth': 5,\n",
    "    'num_rounds': ROUNDS\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_lgb, num_boost_round = ROUNDS)\n",
    "\n",
    "print('Predicting validation and local test file...')\n",
    "pred_valid = model.predict(valid_restr)\n",
    "pred_localtest = model.predict(localtest_restr)\n",
    "print(type(pred_valid))\n",
    "print(pred_valid.shape)\n",
    "validscore = rmsle(valid_y, pred_valid)\n",
    "print('RMSLE on validation set:', validscore)\n",
    "\n",
    "pred_valid = np.exp(pred_valid) - 1\n",
    "pred_valid = pred_valid * valid_restr['full_sq']\n",
    "pred_localtest = np.exp(pred_localtest) - 1\n",
    "pred_localtest = pred_localtest * localtest_restr['full_sq']\n",
    "\n",
    "pred_valid.reset_index(drop = True, inplace = True)\n",
    "valid_id.reset_index(drop = True, inplace = True)\n",
    "pred_localtest.reset_index(drop = True, inplace = True)\n",
    "localtest_id.reset_index(drop = True, inplace = True)\n",
    "out_valid = pd.DataFrame({ID : valid_id, TARGET : pred_valid})\n",
    "out_localtest = pd.DataFrame({ID : localtest_id, TARGET: pred_localtest})\n",
    "\n",
    "# Record validation score:\n",
    "print('Recording scores...')\n",
    "with open(SCORES_FILE, 'a') as outfile:\n",
    "    outfile.write('\\n' + RECORD_SUBMISSION + str(validscore)[2:5] + '_' + str(NUM) + \n",
    "                  '.csv, valid score: ' + str(validscore) + '\\n')\n",
    "\n",
    "# Save predictions for valid:\n",
    "out_valid.to_csv(VALIDPRED_FILE + str(validscore)[2:5] + '_' + str(NUM) +'.csv', index = False)\n",
    "\n",
    "# Save prediction for localtest:\n",
    "out_localtest.to_csv(LOCALTESTPRED_FILE + str(validscore)[2:5] + '_' + str(NUM) +'.csv', \n",
    "                      index = False)\n",
    "\n",
    "print(\"Features importance...\")\n",
    "gain = model.feature_importance('gain')\n",
    "ft = pd.DataFrame({'feature' : model.feature_name(), 'split' : model.feature_importance('split'), \n",
    "                   'gain' : 100 * gain / gain.sum()}).sort_values('gain', ascending = False)\n",
    "print(ft.head(25))\n",
    "\n",
    "plt.figure()\n",
    "ft[['feature', 'gain']].head(25).plot(kind = 'barh', x = 'feature', y = 'gain', legend = False, \n",
    "                                      figsize = (10, 20))\n",
    "plt.gcf().savefig('features_importance' + str(validscore)[2:5] + '_' + str(NUM) + '.png')\n",
    "\n",
    "# Train the final model and create prediction\n",
    "alltrain_restr = pd.concat([train_restr, valid_restr, localtest_restr], ignore_index = True)\n",
    "alltrain_lgb = lgb.Dataset(alltrain_restr, alltrain_y)\n",
    "print('Training model on alltrain dataset and creating predictions to submit...') \n",
    "allmodel = lgb.train(params, alltrain_lgb, num_boost_round = ROUNDS)\n",
    "pred_test = allmodel.predict(test_restr)\n",
    "pred_test = np.exp(pred_test) - 1\n",
    "pred_test = pred_test * test_restr['full_sq']\n",
    "\n",
    "# Predictions to submit:\n",
    "pred_test.reset_index(drop = True, inplace = True)\n",
    "out_test = pd.DataFrame({ID : test_id, TARGET : pred_test})\n",
    "out_test.to_csv(SUBMISSION_FILE + str(validscore)[2:5] + '_' + str(NUM) +'.csv', index = False) \n",
    "                                                                        # do not include index column\n",
    "\n",
    "print(\"Done.\")\n",
    "del alltrain_restr, alltrain_lgb\n",
    "##### public score: 0.31880, private score: 0.32223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
